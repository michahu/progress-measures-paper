{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["__all__ = [\n","    \"Config\",\n","    \"HookPoint\",\n","    \"Embed\",\n","    \"Unembed\",\n","    \"PosEmbed\",\n","    \"LayerNorm\",\n","    \"Attention\",\n","    \"MLP\",\n","    \"TransformerBlock\",\n","    \"Transformer\",\n","    \"make_fourier_basis\",\n","    \"calculate_key_freqs\",\n","    \"get_components_of_trig_loss\",\n","    \"calculate_excluded_loss\",\n","    \"calculate_trig_loss\",\n","    \"calculate_coefficients\",\n","    \"gen_train_test\",\n","    \"full_loss\",\n","    \"Trainer\",\n","    \"train_model\",\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch as t\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import torch.nn.functional as F\n","import einops\n","import random\n","from . import helpers\n","from dataclasses import dataclass\n","import os\n","import wandb\n","\n","import dataclasses\n","from collections import defaultdict"]},{"cell_type":"markdown","metadata":{},"source":["%% ../transformer.ipynb 4<br>\n","TODO does dataclass really require type annotations lol<br>\n","TODO write a hook that also saves gradients"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@dataclass(frozen=True)\n","class Config:\n","    lr: float = 1e-3  # @param\n","    weight_decay: float = 1.0  # @param\n","    p: int = 113  # @param\n","    d_model: int = 128  # @param\n","    fn_name: str = \"add\"  # @param ['add', 'subtract', 'x2xyy2','rand']\n","    frac_train: float = 0.3  # @param\n","    num_epochs: int = 50000  # @param\n","    save_models: bool = False  # @param\n","    save_every: int = 100  # @param # this is good\n","\n","    # TODO for the first 1000 steps, save every 10 because 'interesting stuff happens at the start'\n","    # TODO add a helper function to generate indices here\n","\n","    # Stop training when test loss is <stopping_thresh\n","    stopping_thresh: int = -1  # @param\n","    seed: int = 0  # @param\n","    num_layers: int = 1\n","    batch_style: str = \"full\"\n","    d_vocab: int = p + 1\n","    n_ctx: int = 3\n","    d_mlp: int = 4 * d_model\n","    num_heads: int = 4\n","    act_type: str = \"ReLU\"  # @param ['ReLU', 'GeLU']\n","    device: t.device = t.device(\"cuda\")\n","\n","    # TODO ankify the privileged basis concept- a priori vs etc. ; consider writing up an explanation of privileged basis\n","    use_ln: bool = False\n","    take_metrics_every_n_epochs: int = 1000  # @param\n","    @property\n","    def d_head(self):\n","        return self.d_model // self.num_heads\n","    @property\n","    def random_answers(self):\n","        return np.random.randint(low=0, high=self.p, size=(self.p, self.p))\n","    @property\n","    def fns_dict(self):\n","        return {\n","            \"add\": lambda x, y: (x + y) % self.p,\n","            \"subtract\": lambda x, y: (x - y) % self.p,\n","            \"x2xyy2\": lambda x, y: (x**2 + x * y + y**2) % self.p,\n","            \"rand\": lambda x, y: self.random_answers[x][y],\n","        }\n","    @property\n","    def fn(self):\n","        return self.fns_dict[self.fn_name]\n","    def is_train_is_test(self, train):\n","        \"\"\"Creates an array of Boolean indices according to whether each data point is in train or test.\n","        Used to index into the big batch of all possible data\"\"\"\n","        # TODO probably the wrong place for this\n","        is_train = []\n","        is_test = []\n","        for x in range(self.p):\n","            for y in range(self.p):\n","                if (x, y, 113) in train:\n","                    is_train.append(True)\n","                    is_test.append(False)\n","                else:\n","                    is_train.append(False)\n","                    is_test.append(True)\n","        is_train = np.array(is_train)\n","        is_test = np.array(is_test)\n","        return (is_train, is_test)\n","    def is_it_time_to_save(self, epoch):\n","        return epoch % self.save_every == 0\n","    def is_it_time_to_take_metrics(self, epoch):\n","        return epoch % self.take_metrics_every_n_epochs == 0"]},{"cell_type":"markdown","metadata":{},"source":["TODO make this an assert inside the consturctor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["assert Config.d_model % Config.num_heads == 0"]},{"cell_type":"markdown","metadata":{},"source":["%% ../transformer.ipynb 5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class HookPoint(nn.Module):\n","    \"\"\"A helper class to get access to intermediate activations (inspired by Garcon)\n","    It's a dummy module that is the identity function by default\n","    I can wrap any intermediate activation in a HookPoint and get a convenient way to add PyTorch hooks\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.fwd_hooks = []\n","        self.bwd_hooks = []\n","    def give_name(self, name):\n","        # Called by the model at initialisation\n","        self.name = name\n","    def add_hook(self, hook, dir=\"fwd\"):\n","        # Hook format is fn(activation, hook_name)\n","        # Change it into PyTorch hook format (this includes input and output,\n","        # which are the same for a HookPoint)\n","        def full_hook(module, module_input, module_output):\n","            return hook(module_output, name=self.name)\n","        if dir == \"fwd\":\n","            handle = self.register_forward_hook(full_hook)\n","            self.fwd_hooks.append(handle)\n","        elif dir == \"bwd\":\n","            handle = self.register_backward_hook(full_hook)\n","            self.bwd_hooks.append(handle)\n","        else:\n","            raise ValueError(f\"Invalid direction {dir}\")\n","    def remove_hooks(self, dir=\"fwd\"):\n","        if (dir == \"fwd\") or (dir == \"both\"):\n","            for hook in self.fwd_hooks:\n","                hook.remove()\n","            self.fwd_hooks = []\n","        if (dir == \"bwd\") or (dir == \"both\"):\n","            for hook in self.bwd_hooks:\n","                hook.remove()\n","            self.bwd_hooks = []\n","        if dir not in [\"fwd\", \"bwd\", \"both\"]:\n","            raise ValueError(f\"Invalid direction {dir}\")\n","    def forward(self, x):\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Embed(nn.Module):\n","    \"\"\"Define network architecture\n","    I defined my own transformer from scratch so I'd fully understand each component\n","    - I expect this wasn't necessary or particularly important, and a bunch of this replicates existing Pyt functionality\n","    \"\"\"\n","    def __init__(self, d_vocab, d_model):\n","        super().__init__()\n","        self.W_E = nn.Parameter(t.randn(d_model, d_vocab) / np.sqrt(d_model))\n","    def forward(self, x):\n","        return t.einsum(\"dbp -> bpd\", self.W_E[:, x])\n","    \n","class Unembed(nn.Module):\n","    def __init__(self, d_vocab, d_model):\n","        super().__init__()\n","        self.W_U = nn.Parameter(t.randn(d_model, d_vocab) / np.sqrt(d_vocab))\n","    def forward(self, x):\n","        return x @ self.W_U\n","    \n","class PosEmbed(nn.Module):\n","    def __init__(self, max_ctx, d_model):\n","        super().__init__()\n","        self.W_pos = nn.Parameter(t.randn(max_ctx, d_model) / np.sqrt(d_model))\n","    def forward(self, x):\n","        return x + self.W_pos[: x.shape[-2]]\n","    \n","class LayerNorm(nn.Module):\n","    def __init__(self, d_model, epsilon=1e-4, model=[None]):\n","        super().__init__()\n","        self.model = model\n","        self.w_ln = nn.Parameter(t.ones(d_model))\n","        self.b_ln = nn.Parameter(t.zeros(d_model))\n","        self.epsilon = epsilon\n","    def forward(self, x):\n","        if self.model[0].use_ln:\n","            x = x - x.mean(axis=-1)[..., None]\n","            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n","            x = x * self.w_ln\n","            x = x + self.b_ln\n","            return x\n","        else:\n","            return x\n","\n","class Attention(nn.Module):\n","    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n","        super().__init__()\n","        self.model = model\n","        self.W_K = nn.Parameter(t.randn(num_heads, d_head, d_model) / np.sqrt(d_model))\n","        self.W_Q = nn.Parameter(t.randn(num_heads, d_head, d_model) / np.sqrt(d_model))\n","        self.W_V = nn.Parameter(t.randn(num_heads, d_head, d_model) / np.sqrt(d_model))\n","        self.W_O = nn.Parameter(t.randn(d_model, d_head * num_heads) / np.sqrt(d_model))\n","        self.register_buffer(\"mask\", t.tril(t.ones((n_ctx, n_ctx))))\n","        self.d_head = d_head\n","        self.hook_k = HookPoint()\n","        self.hook_q = HookPoint()\n","        self.hook_v = HookPoint()\n","        self.hook_z = HookPoint()\n","        self.hook_attn = HookPoint()\n","        self.hook_attn_pre = HookPoint()\n","    def forward(self, x):\n","        k = self.hook_k(t.einsum(\"ihd,bpd->biph\", self.W_K, x))\n","        q = self.hook_q(t.einsum(\"ihd,bpd->biph\", self.W_Q, x))\n","        v = self.hook_v(t.einsum(\"ihd,bpd->biph\", self.W_V, x))\n","        attn_scores_pre = t.einsum(\"biph,biqh->biqp\", k, q)\n","        attn_scores_masked = t.tril(attn_scores_pre) - 1e10 * (\n","            1 - self.mask[: x.shape[-2], : x.shape[-2]]\n","        )\n","        attn_matrix = self.hook_attn(\n","            F.softmax(\n","                self.hook_attn_pre(attn_scores_masked / np.sqrt(self.d_head)), dim=-1\n","            )\n","        )\n","        z = self.hook_z(t.einsum(\"biph,biqp->biqh\", v, attn_matrix))\n","        z_flat = einops.rearrange(z, \"b i q h -> b q (i h)\")\n","        out = t.einsum(\"df,bqf->bqd\", self.W_O, z_flat)\n","        return out\n","\n","class MLP(nn.Module):\n","    def __init__(self, d_model, d_mlp, act_type, model):\n","        super().__init__()\n","        self.model = model\n","        self.W_in = nn.Parameter(t.randn(d_mlp, d_model) / np.sqrt(d_model))\n","        self.b_in = nn.Parameter(t.zeros(d_mlp))\n","        self.W_out = nn.Parameter(t.randn(d_model, d_mlp) / np.sqrt(d_model))\n","        self.b_out = nn.Parameter(t.zeros(d_model))\n","        self.act_type = act_type\n","        # self.ln = LayerNorm(d_mlp, model=self.model)\n","        self.hook_pre = HookPoint()\n","        self.hook_post = HookPoint()\n","        assert act_type in [\"ReLU\", \"GeLU\"]\n","    def forward(self, x):\n","        x = self.hook_pre(t.einsum(\"md,bpd->bpm\", self.W_in, x) + self.b_in)\n","        if self.act_type == \"ReLU\":\n","            x = F.relu(x)\n","        elif self.act_type == \"GeLU\":\n","            x = F.gelu(x)\n","        x = self.hook_post(x)\n","        x = t.einsum(\"dm,bpm->bpd\", self.W_out, x) + self.b_out\n","        return x\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n","        super().__init__()\n","        self.model = model\n","        # self.ln1 = LayerNorm(d_model, model=self.model)\n","        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n","        # self.ln2 = LayerNorm(d_model, model=self.model)\n","        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n","        self.hook_attn_out = HookPoint()\n","        self.hook_mlp_out = HookPoint()\n","        self.hook_resid_pre = HookPoint()\n","        self.hook_resid_mid = HookPoint()\n","        self.hook_resid_post = HookPoint()\n","    def forward(self, x):\n","        x = self.hook_resid_mid(\n","            x + self.hook_attn_out(self.attn((self.hook_resid_pre(x))))\n","        )\n","        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n","        return x\n","\n","class Transformer(nn.Module):\n","    def __init__(self, config: Config, use_cache=False, use_ln=True):\n","        \"\"\"this function could be augmented to contain more options for creating different architectures\"\"\"\n","        super().__init__()\n","        self.cache = {}\n","        self.use_cache = use_cache\n","        self.embed = Embed(d_vocab=config.d_vocab, d_model=config.d_model)\n","        self.pos_embed = PosEmbed(max_ctx=config.n_ctx, d_model=config.d_model)\n","        self.blocks = nn.ModuleList(\n","            [\n","                TransformerBlock(\n","                    d_model=config.d_model,\n","                    d_mlp=config.d_mlp,\n","                    d_head=config.d_head,\n","                    num_heads=config.num_heads,\n","                    n_ctx=config.n_ctx,\n","                    act_type=config.act_type,\n","                    model=[self],\n","                )\n","                for i in range(config.num_layers)\n","            ]\n","        )\n","        self.unembed = Unembed(d_vocab=config.d_vocab, d_model=config.d_model)\n","        self.use_ln = use_ln\n","        for name, module in self.named_modules():\n","            if type(module) == HookPoint:\n","                module.give_name(name)\n","    def forward(self, x):\n","        x = self.embed(x)\n","        x = self.pos_embed(x)\n","        for block in self.blocks:\n","            x = block(x)\n","        # x = self.ln(x)\n","        x = self.unembed(x)\n","        return x\n","    def set_use_cache(self, use_cache):\n","        self.use_cache = use_cache\n","    def hook_points(self):\n","        return [module for name, module in self.named_modules() if \"hook\" in name]\n","    def remove_all_hooks(self):\n","        for hp in self.hook_points():\n","            hp.remove_hooks(\"fwd\")\n","            hp.remove_hooks(\"bwd\")\n","    def cache_all(self, cache, incl_bwd=False):\n","        # Caches all activations wrapped in a HookPoint\n","        def save_hook(tensor, name):\n","            cache[name] = tensor.detach()\n","        def save_hook_back(tensor, name):\n","            cache[name + \"_grad\"] = tensor[0].detach()\n","        for hp in self.hook_points():\n","            hp.add_hook(save_hook, \"fwd\")\n","            if incl_bwd:\n","                hp.add_hook(save_hook_back, \"bwd\")\n","\n","def make_fourier_basis(config: Config):\n","    fourier_basis = []\n","    fourier_basis.append(t.ones(config.p) / np.sqrt(config.p))\n","    fourier_basis_names = [\"Const\"]\n","    # Note that if p is even, we need to explicitly add a term for cos(kpi), ie\n","    # alternating +1 and -1\n","    for i in range(1, config.p // 2 + 1):\n","        fourier_basis.append(t.cos(2 * t.pi * t.arange(config.p) * i / config.p))\n","        fourier_basis.append(t.sin(2 * t.pi * t.arange(config.p) * i / config.p))\n","        fourier_basis[-2] /= fourier_basis[-2].norm()\n","        fourier_basis[-1] /= fourier_basis[-1].norm()\n","        fourier_basis_names.append(f\"cos {i}\")\n","        fourier_basis_names.append(f\"sin {i}\")\n","    return t.stack(fourier_basis, dim=0).to(config.device)\n","\n","def calculate_key_freqs(config: Config, model: Transformer, all_data):\n","    # TODO this was moved from the app code; probably move it around\n","    labels = t.tensor([config.fn(i, j) for i, j, _ in all_data]).to(config.device)\n","    cache = {}\n","    model.remove_all_hooks()  # TODO is this line fucky??\n","    model.cache_all(cache)\n","    model(all_data)\n","    neuron_acts = cache[\"blocks.0.mlp.hook_post\"][:, -1]\n","    # Center the neurons to remove the constant term\n","    neuron_acts_centered = neuron_acts - einops.reduce(\n","        neuron_acts, \"batch neuron -> 1 neuron\", \"mean\"\n","    )\n","    # Note that fourier_neuron_acts[(0, 0), i]==0 for all i, because we centered the activations\n","    fourier_basis = make_fourier_basis(config=config)\n","    fourier_neuron_acts = helpers.fft2d(\n","        neuron_acts_centered, p=config.p, fourier_basis=fourier_basis\n","    )\n","    fourier_neuron_acts_square = fourier_neuron_acts.reshape(\n","        config.p, config.p, config.d_mlp\n","    )\n","    neuron_freqs = []\n","    neuron_frac_explained = []\n","    for ni in range(config.d_mlp):\n","        best_frac_explained = -1e6\n","        best_freq = -1\n","        for freq in range(1, config.p // 2):\n","            # We extract the linear and quadratic fourier terms of frequency freq,\n","            # and look at how much of the variance of the full vector this explains\n","            # If neurons specialise into specific frequencies, one frequency should\n","            # have a large value\n","            numerator = (\n","                helpers.extract_freq_2d(\n","                    fourier_neuron_acts_square[:, :, ni], freq, p=config.p\n","                )\n","                .pow(2)\n","                .sum()\n","            )\n","            denominator = fourier_neuron_acts_square[:, :, ni].pow(2).sum().item()\n","            frac_explained = numerator / denominator\n","            if frac_explained > best_frac_explained:\n","                best_freq = freq\n","                best_frac_explained = frac_explained\n","        neuron_freqs.append(best_freq)\n","        neuron_frac_explained.append(best_frac_explained)\n","    neuron_freqs = np.array(neuron_freqs)\n","    neuron_frac_explained = helpers.to_numpy(neuron_frac_explained)\n","    key_freqs, neuron_freq_counts = np.unique(neuron_freqs, return_counts=True)\n","    return key_freqs\n","\n","def get_components_of_trig_loss(logits, freq, fourier_basis):\n","    cos = helpers.get_component_cos_xpy(logits, freq, fourier_basis=fourier_basis)\n","    sin = helpers.get_component_sin_xpy(logits, freq, fourier_basis=fourier_basis)\n","    return cos + sin\n","\n","def calculate_excluded_loss(\n","    config: Config, fourier_basis, key_freqs, is_train, is_test, labels, logits\n","):\n","    row = []\n","    for freq in key_freqs:\n","        cos = helpers.get_component_cos_xpy(logits, freq, fourier_basis=fourier_basis)\n","        sin = helpers.get_component_sin_xpy(logits, freq, fourier_basis=fourier_basis)\n","        value = helpers.test_logits(\n","            logits - cos - sin,\n","            bias_correction=False,\n","            mode=\"train\",\n","            p=config.p,\n","            is_train=is_train,\n","            is_test=is_test,\n","            labels=labels,\n","        )\n","        row.append(value.item())\n","    return row\n","\n","def calculate_trig_loss(\n","    config: Config,\n","    model,\n","    train,\n","    logits,\n","    key_freqs,\n","    fourier_basis,\n","    all_data,\n","    is_train,\n","    is_test,\n","    labels,\n","    mode=\"all\",\n","):\n","    trig_logits = sum(\n","        [get_components_of_trig_loss(logits, freq, fourier_basis) for freq in key_freqs]\n","    )\n","    return helpers.test_logits(\n","        trig_logits,\n","        p=config.p,\n","        is_train=is_train,\n","        is_test=is_test,\n","        labels=labels,\n","        bias_correction=True,\n","        original_logits=logits,\n","        mode=mode,\n","    )\n","\n","def calculate_coefficients(logits, fourier_basis, key_freqs, p, device):\n","    \"\"\"updated version from https://colab.research.google.com/drive/1ScVRL8OCtTFpOHpgfz0PLTFvX4g_YbuN?usp=sharing#scrollTo=WY4nPUDwl9UN\"\"\"\n","    x = t.arange(p)[None, :, None, None]\n","    y = t.arange(p)[None, None, :, None]\n","    z = t.arange(p)[None, None, None, :]\n","    w = t.arange(1, (p // 2 + 1))[:, None, None, None]\n","    coses = t.cos(w * t.pi * 2 / p * (x + y - z)).to(device)\n","    coses = coses.reshape(p // 2, p * p, p)\n","    coses /= coses.pow(2).sum([-2, -1], keepdim=True).sqrt()\n","    cos_coefficients = (coses * logits).sum([-2, -1])\n","    return cos_coefficients\n","\n","def gen_train_test(config: Config):\n","    \"\"\"Generate train and test split\"\"\"\n","    num_to_generate = config.p\n","    pairs = [\n","        (i, j, num_to_generate)\n","        for i in range(num_to_generate)\n","        for j in range(num_to_generate)\n","    ]\n","    random.seed(config.seed)\n","    random.shuffle(pairs)\n","    div = int(config.frac_train * len(pairs))\n","    return pairs[:div], pairs[div:]\n","\n","def full_loss(config: Config, model: Transformer, data):\n","    \"\"\"Takes the cross entropy loss of the model on the data\"\"\"\n","    # Take the final position only\n","    logits = model(data)[:, -1]\n","    labels = t.tensor([config.fn(i, j) for i, j, _ in data]).to(config.device)\n","    return helpers.cross_entropy_high_precision(logits, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Trainer:\n","    \"\"\"TODO\n","    ways this stinks:\n","    - callbacks every k epochs\n","    - training on infinite data\n","    - general abstract class w/o assumption and subclasses w/ more assumptions\n","    - check out hugging face trainer\n","    - disentangle optimization step and taking gradients\n","    - forward compatibility, e.g. batches per step\n","    \"\"\"\n","    def __init__(self, config: Config, model=None) -> None:\n","        wandb.init(project=\"grokking\", config=dataclasses.asdict(config))\n","        self.model = (\n","            model if model is not None else Transformer(config, use_cache=False)\n","        )\n","        self.model.to(config.device)\n","        self.optimizer = optim.AdamW(\n","            self.model.parameters(),\n","            lr=config.lr,\n","            weight_decay=config.weight_decay,\n","            betas=(0.9, 0.98),\n","        )\n","        self.scheduler = optim.lr_scheduler.LambdaLR(\n","            self.optimizer, lambda step: min(step / 10, 1)\n","        )  # TODO make this a config option\n","        self.run_name = f\"grok_{int(time.time())}\"\n","        self.train, self.test = gen_train_test(config=config)\n","        self.metrics_dictionary = defaultdict(\n","            dict\n","        )  # so we can safely call 'update' on keys\n","        print(\"training length = \", len(self.train))\n","        print(\"testing length = \", len(self.test))\n","        self.train_losses = []\n","        self.test_losses = []\n","        self.config = config\n","    def save_epoch(self, epoch, save_to_wandb=True):\n","        \"\"\"precondition! train loss and test losses have been appended to\"\"\"\n","        save_dict = {\n","            \"model\": self.model.state_dict(),\n","            \"train_loss\": self.train_losses[-1],\n","            \"test_loss\": self.test_losses[-1],\n","            \"epoch\": epoch,\n","            # also add gradient here\n","        }\n","        if save_to_wandb:\n","            wandb.log(save_dict)\n","            print(\"Saved epoch to wandb\")\n","        if self.config.save_models:\n","            t.save(save_dict, helpers.root / self.run_name / f\"{epoch}.pth\")\n","            print(f\"Saved model to {helpers.root/self.run_name/f'{epoch}.pth'}\")\n","        self.metrics_dictionary[epoch].update(save_dict)\n","    def do_a_training_step(self, epoch: int):\n","        \"\"\"returns train_loss, test_loss\"\"\"\n","        train_loss = full_loss(config=self.config, model=self.model, data=self.train)\n","        test_loss = full_loss(config=self.config, model=self.model, data=self.test)\n","        self.train_losses.append(train_loss.item())\n","        self.test_losses.append(test_loss.item())\n","        if epoch % 100 == 0:\n","            # TODO is this ok? this was np.log, and it was barking at me ; i think np.log was being interpreted as a logging module\n","            print(\n","                f\"Epoch {epoch}, train loss {t.log(train_loss).item():.4f}, test loss {t.log(test_loss).item():.4f}\"\n","            )\n","        train_loss.backward()\n","        self.optimizer.step()\n","        self.scheduler.step()\n","        self.optimizer.zero_grad()\n","        return train_loss, test_loss\n","    def initial_save_if_appropriate(self):\n","        if self.config.save_models:\n","            os.mkdir(helpers.root / self.run_name)\n","            save_dict = {\n","                \"model\": self.model.state_dict(),\n","                \"train_data\": self.train,\n","                \"test_data\": self.test,\n","            }\n","            t.save(save_dict, helpers.root / self.run_name / \"init.pth\")\n","    def post_training_save(self, save_optimizer_and_scheduler=True, log_to_wandb=True):\n","        if not self.config.save_models:\n","            os.makedirs(helpers.root / self.run_name, exist_ok=True)\n","        save_dict = {\n","            \"model\": self.model.state_dict(),\n","            \"train_loss\": self.train_losses[-1],\n","            \"test_loss\": self.test_losses[-1],\n","            \"train_losses\": self.train_losses,\n","            \"test_losses\": self.test_losses,\n","            \"epoch\": self.config.num_epochs,\n","        }\n","        if save_optimizer_and_scheduler:\n","            save_dict[\"optimizer\"] = self.optimizer.state_dict()\n","            save_dict[\"scheduler\"] = self.scheduler.state_dict()\n","        if log_to_wandb:\n","            wandb.log(save_dict)\n","        t.save(save_dict, helpers.root / self.run_name / f\"final.pth\")\n","        print(f\"Saved model to {helpers.root/self.run_name/f'final.pth'}\")\n","        self.metrics_dictionary[save_dict[\"epoch\"]].update(save_dict)\n","    def take_metrics(self, train, epoch):\n","        with t.inference_mode():\n","            def sum_sq_weights():\n","                # TODO refactor- taken from app code\n","                row = []\n","                for name, param in self.model.named_parameters():\n","                    row.append(param.pow(2).sum().item())\n","                return row\n","            print(\"taking metrics\")\n","            all_data = t.tensor(\n","                [\n","                    (i, j, self.config.p)\n","                    for i in range(self.config.p)\n","                    for j in range(self.config.p)\n","                ]\n","            ).to(self.config.device)\n","            # TODO calculate key freqs is the most expensive part of this\n","            key_freqs = calculate_key_freqs(\n","                config=self.config, model=self.model, all_data=all_data\n","            )\n","            logits = self.model(all_data)[\n","                :, -1, :-1\n","            ]  # TODO i think this is equivalent to what's in the new paper?\n","            fourier_basis = make_fourier_basis(config=self.config)\n","            is_train, is_test = self.config.is_train_is_test(train=train)\n","            labels = t.tensor([self.config.fn(i, j) for i, j, _ in all_data]).to(\n","                self.config.device\n","            )\n","            metrics = {\n","                \"epoch\": epoch,\n","                \"trig_loss\": calculate_trig_loss(\n","                    config=self.config,\n","                    model=self.model,\n","                    train=train,\n","                    key_freqs=key_freqs,\n","                    is_test=is_test,\n","                    is_train=is_train,\n","                    labels=labels,\n","                    logits=logits,\n","                    fourier_basis=fourier_basis,\n","                    all_data=all_data,\n","                ),\n","                \"sum_of_squared_weights\": sum_sq_weights(),\n","                \"excluded_loss\": calculate_excluded_loss(\n","                    logits=logits,\n","                    key_freqs=key_freqs,\n","                    fourier_basis=fourier_basis,\n","                    is_train=is_train,\n","                    config=self.config,\n","                    is_test=is_test,\n","                    labels=labels,\n","                ),\n","                \"coefficients\": calculate_coefficients(\n","                    p=self.config.p,\n","                    logits=logits,\n","                    fourier_basis=fourier_basis,\n","                    key_freqs=key_freqs,\n","                    device=self.config.device,\n","                ),\n","            }\n","            wandb.log(metrics)\n","            print(\"Logged metrics to wandb\")\n","            self.metrics_dictionary[epoch].update(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(config: Config):\n","    world = Trainer(config=config)\n","    print(f\"Run name {world.run_name}\")\n","    world.initial_save_if_appropriate()\n","    for epoch in range(config.num_epochs):\n","        train_loss, test_loss = world.do_a_training_step(epoch)\n","        if test_loss.item() < config.stopping_thresh:\n","            break\n","        if config.is_it_time_to_save(epoch=epoch):\n","            # TODO this also used to do a check about test loss- pretty sure not necessary\n","            world.save_epoch(epoch=epoch)\n","        if config.is_it_time_to_take_metrics(epoch=epoch):\n","            world.take_metrics(epoch=epoch, train=world.train)\n","    world.post_training_save(save_optimizer_and_scheduler=True)\n","    helpers.lines(\n","        [world.train_losses, world.test_losses], labels=[\"train\", \"test\"], log_y=True\n","    )\n","    return world  # to export the dictionary with the training metrics"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"2be9eef5c721069f4839ed9a1dbcfe4377c9c422b1534c2c9a8d9b339140427f"}}},"nbformat":4,"nbformat_minor":2}
